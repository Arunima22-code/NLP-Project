{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mXvOri55Q6kP",
        "outputId": "1a417dfc-b3c7-4373-fffa-cfd079315b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.11.0\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "Installing collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.7 starlette-0.45.3 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install spacy\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install flask\n",
        "!pip install rapidfuzz\n",
        "!pip install fastapi uvicorn  # only if you want to test API code locally in Colab or you plan to run it on a VM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install optuna\n",
        "!pip install torch\n",
        "!pip install spacy\n",
        "!pip install iterative-stratification\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o4kA0fkjRW71",
        "outputId": "a63b53cf-de1c-4cef-b9e2-0e6d5c3e018c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.5.0)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Download NLTK data if not already present\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # For WordNet lemmatizer\n",
        "nltk.download('punkt_tab')  # Explicit download for punkt_tab\n",
        "\n",
        "# Load spaCy model for additional preprocessing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define domain knowledge\n",
        "domain_knowledge = {\n",
        "    \"competitors\": [\n",
        "        \"CompetitorX\", \"CompetitorY\", \"AcmeCorp\", \"NextGenTech\", \"CloudSolutions\",\n",
        "        \"DataInsights\", \"AutoMetrics\", \"HyperCloud\", \"InfoTech\", \"QuantumSoft\"\n",
        "    ],\n",
        "    \"features\": [\n",
        "        \"real-time reporting\", \"automation suite\", \"advanced metrics\", \"AI engine\",\n",
        "        \"data pipeline\", \"compliance dashboard\", \"enterprise-grade analytics\",\n",
        "        \"scalable architecture\", \"cloud optimization\"\n",
        "    ],\n",
        "    \"pricing_keywords\": [\n",
        "        \"discount\", \"promo code\", \"rebate\", \"cost reduction\", \"special rate\",\n",
        "        \"bulk pricing\", \"early sign-up offer\", \"pricing model\", \"cost efficiency\"\n",
        "    ],\n",
        "    \"compliance\": [\n",
        "        \"SOC2\", \"FedRAMP\", \"PCI-DSS\", \"ISO 27001\", \"HIPAA\", \"GDPR\",\n",
        "        \"CCPA\", \"NIST compliance\", \"data privacy\", \"risk assessment\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define snippet templates\n",
        "snippet_templates = [\n",
        "    \"We are impressed with {feature}, but {competitor} offers a better {pricing}.\",\n",
        "    \"Does your solution handle {compliance}? {competitor} seems to.\",\n",
        "    \"Our finance department is asking for a {pricing} or we'll stick with {competitor}.\",\n",
        "    \"We are concerned about {compliance}.\",\n",
        "    \"Can you provide a {pricing} if we commit early?\",\n",
        "    \"How does your solution compare to {competitor} in terms of {feature}?\",\n",
        "    \"We need better {feature} to handle enterprise demands.\",\n",
        "    \"Does your automation suite support {compliance} and other requirements?\",\n",
        "    \"Competitor {competitor} offers better pricing. How can you match it?\",\n",
        "    \"We are worried about pricing. Can you provide a better {pricing}?\"\n",
        "]\n",
        "\n",
        "# Generate realistic text snippets\n",
        "def generate_snippet(template, domain_knowledge):\n",
        "    return template.format(\n",
        "        feature=random.choice(domain_knowledge[\"features\"]),\n",
        "        competitor=random.choice(domain_knowledge[\"competitors\"]),\n",
        "        pricing=random.choice(domain_knowledge[\"pricing_keywords\"]),\n",
        "        compliance=random.choice(domain_knowledge[\"compliance\"])\n",
        "    )\n",
        "\n",
        "# Generate a diverse dataset\n",
        "def create_dataset(num_samples=300):\n",
        "    data = []\n",
        "    labels_map = {\n",
        "        \"Positive\": [\"impressed\", \"love\", \"better feature\"],\n",
        "        \"Negative\": [\"concerned\", \"worried\", \"stick with\"],\n",
        "        \"Objection\": [\"match\", \"handle\", \"requirements\"],\n",
        "        \"Pricing Discussion\": [\"discount\", \"pricing\", \"rebate\", \"promo code\"],\n",
        "        \"Compliance\": [\"SOC2\", \"FedRAMP\", \"PCI-DSS\", \"GDPR\"],\n",
        "        \"Competition\": [\"compare\", \"Competitor\"]\n",
        "    }\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        template = random.choice(snippet_templates)\n",
        "        snippet = generate_snippet(template, domain_knowledge)\n",
        "\n",
        "        # Assign labels based on keywords\n",
        "        labels = [\n",
        "            label for label, keywords in labels_map.items()\n",
        "            if any(keyword in snippet for keyword in keywords)\n",
        "        ]\n",
        "        if not labels:\n",
        "            labels = [\"Uncategorized\"]\n",
        "\n",
        "        data.append({\"text_snippet\": snippet, \"labels\": \", \".join(labels)})\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Preprocess text: cleaning, lemmatization, and stopword removal\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to dataset\n",
        "def preprocess_dataset(df):\n",
        "    df[\"text_snippet_cleaned\"] = df[\"text_snippet\"].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "# Handle label imbalance by oversampling\n",
        "def balance_labels(df):\n",
        "    label_counts = Counter(df[\"labels\"])\n",
        "    min_count = max(label_counts.values())  # Balance all labels to the max count\n",
        "\n",
        "    balanced_df = pd.DataFrame()\n",
        "    for label, count in label_counts.items():\n",
        "        label_df = df[df[\"labels\"] == label]\n",
        "        if count < min_count:\n",
        "            label_df = resample(label_df, replace=True, n_samples=min_count, random_state=42)\n",
        "        balanced_df = pd.concat([balanced_df, label_df])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Save domain knowledge to JSON\n",
        "def save_domain_knowledge(domain_knowledge):\n",
        "    with open(\"domain_knowledge.json\", \"w\") as f:\n",
        "        json.dump(domain_knowledge, f, indent=4)\n",
        "    print(\"Domain knowledge JSON saved.\")\n",
        "\n",
        "# Main function to create, preprocess, and save the dataset\n",
        "def main():\n",
        "    # Generate dataset\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = create_dataset()\n",
        "    print(f\"Generated {len(dataset)} samples.\")\n",
        "\n",
        "    # Preprocess dataset\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = preprocess_dataset(dataset)\n",
        "\n",
        "    # Balance labels\n",
        "    print(\"Balancing labels...\")\n",
        "    dataset = balance_labels(dataset)\n",
        "\n",
        "    # Save dataset\n",
        "    dataset.to_csv(\"calls_dataset.csv\", index=False)\n",
        "    print(\"Dataset saved as 'calls_dataset.csv'.\")\n",
        "\n",
        "    # Save domain knowledge\n",
        "    save_domain_knowledge(domain_knowledge)\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9YcNOQK3Rw8D",
        "outputId": "acd1d72c-e02b-448a-fc2e-8a504e6f5c8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Generated 300 samples.\n",
            "Preprocessing dataset...\n",
            "Balancing labels...\n",
            "Dataset saved as 'calls_dataset.csv'.\n",
            "Domain knowledge JSON saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import optuna\n",
        "from typing import List, Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "# Preprocessor Class\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        # Lowercase, remove special characters, and tokenize\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "# Dataset Class\n",
        "class MultiLabelDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Classifier Class\n",
        "class TransformerMultiLabelClassifier:\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=6):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"multi_label_classification\"\n",
        "        )\n",
        "\n",
        "    def train_with_optuna(self, train_texts, train_labels, val_texts, val_labels):\n",
        "        def objective(trial):\n",
        "            # Suggest hyperparameters\n",
        "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
        "            batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "            # Create data loaders\n",
        "            train_dataset = MultiLabelDataset(train_texts, train_labels, self.tokenizer)\n",
        "            val_dataset = MultiLabelDataset(val_texts, val_labels, self.tokenizer)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "            # Optimizer and scheduler\n",
        "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            for epoch in range(1):  # Reduced to 1 epoch for faster tuning\n",
        "                for batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.model(\n",
        "                        input_ids=batch[\"input_ids\"],\n",
        "                        attention_mask=batch[\"attention_mask\"],\n",
        "                        labels=batch[\"labels\"]\n",
        "                    )\n",
        "                    loss = outputs.loss\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            val_preds, val_labels_flat = [], []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    outputs = self.model(\n",
        "                        input_ids=batch[\"input_ids\"],\n",
        "                        attention_mask=batch[\"attention_mask\"]\n",
        "                    )\n",
        "                    val_preds.extend(torch.sigmoid(outputs.logits).cpu().numpy())\n",
        "                    val_labels_flat.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "            # F1 Score\n",
        "            val_f1 = f1_score(\n",
        "                val_labels_flat,\n",
        "                (np.array(val_preds) > 0.5),\n",
        "                average=\"weighted\"\n",
        "            )\n",
        "            return val_f1\n",
        "\n",
        "        # Run Optuna optimization\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=3)  # Reduced to 3 trials\n",
        "        return study.best_params\n",
        "\n",
        "    def save(self, path=\"transformer_multi_label_classifier.pkl\"):\n",
        "        joblib.dump(self.model, path)\n",
        "\n",
        "# Training Pipeline\n",
        "def train_pipeline(data_path=\"calls_dataset.csv\"):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(data_path)\n",
        "    preprocessor = TextPreprocessor()\n",
        "    df[\"cleaned_text\"] = df[\"text_snippet\"].apply(preprocessor.preprocess)\n",
        "\n",
        "    # Split data\n",
        "    texts = df[\"cleaned_text\"].tolist()\n",
        "    labels = df[\"labels\"].str.get_dummies(sep=\", \").values.tolist()\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize and train classifier\n",
        "    classifier = TransformerMultiLabelClassifier(num_labels=len(labels[0]))\n",
        "    best_params = classifier.train_with_optuna(train_texts, train_labels, val_texts, val_labels)\n",
        "    print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "    # Save the model\n",
        "    classifier.save()\n",
        "    print(\"Model saved as 'transformer_multi_label_classifier.pkl'\")\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    train_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MaQpTMY7ZrXL",
        "outputId": "6a5a83a1-9905-4182-dc89-ab5c78361897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-01-26 15:58:28,789] A new study created in memory with name: no-name-4f170b9d-0c04-4ff3-9701-de9c3214150d\n",
            "<ipython-input-12-b1f165f642b0>:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
            "[I 2025-01-26 16:07:51,087] Trial 0 finished with value: 0.7395352384213305 and parameters: {'learning_rate': 3.000117376949386e-05, 'batch_size': 32}. Best is trial 0 with value: 0.7395352384213305.\n",
            "<ipython-input-12-b1f165f642b0>:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
            "[I 2025-01-26 16:17:40,871] Trial 1 finished with value: 0.8248727163821503 and parameters: {'learning_rate': 1.6144628511646174e-05, 'batch_size': 32}. Best is trial 1 with value: 0.8248727163821503.\n",
            "<ipython-input-12-b1f165f642b0>:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from typing import Dict, List, Any\n",
        "from rapidfuzz import process\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "class AdvancedEntityExtractor:\n",
        "    def __init__(self, domain_knowledge_path: str):\n",
        "        # Load spaCy model\n",
        "        self.nlp = spacy.load(\"en_core_web_trf\")  # Transformer-based spaCy model\n",
        "        # Load domain knowledge\n",
        "        with open(domain_knowledge_path, \"r\") as f:\n",
        "            self.domain_knowledge = json.load(f)\n",
        "        # Sentence Transformer for semantic matching\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def semantic_matching(self, text: str, candidates: List[str], threshold: float = 0.7) -> List[str]:\n",
        "        text_embedding = self.embedding_model.encode(text)\n",
        "        candidate_embeddings = self.embedding_model.encode(candidates)\n",
        "        similarities = [\n",
        "            np.dot(text_embedding, candidate_emb) /\n",
        "            (np.linalg.norm(text_embedding) * np.linalg.norm(candidate_emb))\n",
        "            for candidate_emb in candidate_embeddings\n",
        "        ]\n",
        "        return [candidates[i] for i, sim in enumerate(similarities) if sim > threshold]\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        entities = {\n",
        "            \"competitors\": [],\n",
        "            \"features\": [],\n",
        "            \"pricing_keywords\": [],\n",
        "            \"compliance\": [],\n",
        "            \"ner_entities\": []\n",
        "        }\n",
        "\n",
        "        # Exact Matching\n",
        "        for category, terms in self.domain_knowledge.items():\n",
        "            for term in terms:\n",
        "                if term.lower() in text.lower():\n",
        "                    entities[category].append(term)\n",
        "\n",
        "        # Fuzzy Matching\n",
        "        for category in [\"competitors\", \"pricing_keywords\"]:\n",
        "            matches = process.extract(\n",
        "                text, self.domain_knowledge.get(category, []), scorer=process.fuzz.partial_ratio\n",
        "            )\n",
        "            entities[category].extend([match[0] for match in matches if match[1] > 80])\n",
        "\n",
        "        # Semantic Matching\n",
        "        for category in [\"competitors\", \"features\"]:\n",
        "            semantic_matches = self.semantic_matching(text, self.domain_knowledge.get(category, []))\n",
        "            entities[category].extend(semantic_matches)\n",
        "\n",
        "        # NER Extraction\n",
        "        doc = self.nlp(text)\n",
        "        entities[\"ner_entities\"] = [\n",
        "            ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"MONEY\"]\n",
        "        ]\n",
        "\n",
        "        # Deduplicate and return\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "        return entities\n",
        "\n",
        "\n",
        "class AdvancedTextSummarizer:\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        self.summarizer = pipeline(\"summarization\", model=self.model, tokenizer=self.tokenizer, device=0)\n",
        "\n",
        "    def dynamic_summarization(self, text: str, entities: Dict[str, List[str]], max_length: int = None) -> str:\n",
        "        text_length = len(text.split())\n",
        "        max_length = max_length or min(100, max(30, int(text_length * 0.3)))\n",
        "        min_length = max(20, int(text_length * 0.1))\n",
        "        try:\n",
        "            summary = self.summarizer(\n",
        "                text, max_length=max_length, min_length=min_length, do_sample=False\n",
        "            )[0][\"summary_text\"]\n",
        "        except Exception as e:\n",
        "            summary = \"Summary generation failed.\"\n",
        "\n",
        "        entity_summary = \" | \".join([f\"{k}: {', '.join(v)}\" for k, v in entities.items() if v])\n",
        "        return f\"{summary} | {entity_summary}\"\n",
        "\n",
        "\n",
        "def process_text(text: str, domain_knowledge_path: str) -> Dict[str, Any]:\n",
        "    extractor = AdvancedEntityExtractor(domain_knowledge_path)\n",
        "    summarizer = AdvancedTextSummarizer()\n",
        "    entities = extractor.extract_entities(text)\n",
        "    summary = summarizer.dynamic_summarization(text, entities)\n",
        "    return {\"original_text\": text, \"entities\": entities, \"summary\": summary}\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    domain_knowledge = {\n",
        "        \"competitors\": [\"CompetitorX\", \"CompetitorY\", \"AcmeCorp\"],\n",
        "        \"features\": [\"real-time analytics\", \"automation suite\", \"advanced metrics\"],\n",
        "        \"pricing_keywords\": [\"discount\", \"cost reduction\", \"pricing model\"],\n",
        "        \"compliance\": [\"GDPR\", \"CCPA\", \"SOC2\"]\n",
        "    }\n",
        "\n",
        "    with open(\"domain_knowledge.json\", \"w\") as f:\n",
        "        json.dump(domain_knowledge, f)\n",
        "\n",
        "    text_snippet = (\n",
        "        \"CompetitorX offers advanced real-time analytics at a lower pricing model. \"\n",
        "        \"We need cost reduction to match their capabilities.\"\n",
        "    )\n",
        "    result = process_text(text_snippet, \"domain_knowledge.json\")\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udos8PlX_V8w",
        "outputId": "dd814d60-099c-4cf2-a9f9-b8c5c6e9b6e6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dataset saved to 'augmented_calls_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Dict, Any\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import spacy\n",
        "\n",
        "class AdvancedInferencePipeline:\n",
        "    def __init__(self,\n",
        "                 classifier_path: str,\n",
        "                 domain_knowledge_path: str,\n",
        "                 summary_model: str = \"facebook/bart-large-cnn\"):\n",
        "        # Optimized model loading\n",
        "        self.classifier = joblib.load(classifier_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        # Advanced embedding model\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Load domain knowledge with caching\n",
        "        with open(domain_knowledge_path, \"r\") as f:\n",
        "            self.domain_knowledge = json.load(f)\n",
        "\n",
        "        # Precompute embeddings for faster semantic matching\n",
        "        self.precomputed_embeddings = {\n",
        "            category: self.embedding_model.encode(terms)\n",
        "            for category, terms in self.domain_knowledge.items()\n",
        "            if isinstance(terms, list)\n",
        "        }\n",
        "\n",
        "        # Lightweight spaCy model\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'tagger'])\n",
        "\n",
        "    def predict_labels(self, text: str, threshold: float = 0.5) -> Dict[str, float]:\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.classifier(**inputs)\n",
        "            probabilities = torch.sigmoid(outputs.logits).numpy()[0]\n",
        "\n",
        "        return {\n",
        "            f\"Label_{i}\": float(prob)\n",
        "            for i, prob in enumerate(probabilities)\n",
        "            if prob > threshold\n",
        "        }\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, Any]:\n",
        "        entities = {\n",
        "            \"competitors\": [],\n",
        "            \"features\": [],\n",
        "            \"pricing_keywords\": [],\n",
        "            \"compliance\": [],\n",
        "            \"ner_entities\": []\n",
        "        }\n",
        "\n",
        "        # Concurrent extraction methods\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # 1. Exact Matching (Fast)\n",
        "        for category, terms in self.domain_knowledge.items():\n",
        "            if isinstance(terms, list):\n",
        "                entities[category].extend([\n",
        "                    term for term in terms\n",
        "                    if term.lower() in text_lower\n",
        "                ])\n",
        "\n",
        "        # 2. Fuzzy Matching\n",
        "        fuzzy_categories = [\"competitors\", \"pricing_keywords\"]\n",
        "        for category in fuzzy_categories:\n",
        "            matches = process.extract(\n",
        "                text,\n",
        "                self.domain_knowledge.get(category, []),\n",
        "                scorer=process.fuzz.partial_ratio,\n",
        "                limit=3\n",
        "            )\n",
        "            entities[category].extend([match[0] for match in matches if match[1] > 80])\n",
        "\n",
        "        # 3. Semantic Matching (Optimized)\n",
        "        semantic_categories = [\"competitors\", \"features\"]\n",
        "        text_embedding = self.embedding_model.encode(text)\n",
        "\n",
        "        for category in semantic_categories:\n",
        "            candidates = self.domain_knowledge.get(category, [])\n",
        "            candidate_embeddings = self.precomputed_embeddings.get(category, [])\n",
        "\n",
        "            similarities = [\n",
        "                np.dot(text_embedding, candidate_emb) /\n",
        "                (np.linalg.norm(text_embedding) * np.linalg.norm(candidate_emb))\n",
        "                for candidate_emb in candidate_embeddings\n",
        "            ]\n",
        "\n",
        "            entities[category].extend([\n",
        "                candidates[i] for i, sim in enumerate(similarities) if sim > 0.7\n",
        "            ])\n",
        "\n",
        "        # 4. NER Extraction\n",
        "        doc = self.nlp(text)\n",
        "        entities[\"ner_entities\"] = [\n",
        "            ent.text for ent in doc.ents\n",
        "            if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"MONEY\"]\n",
        "        ]\n",
        "\n",
        "        # Deduplicate results\n",
        "        return {k: list(set(v)) for k, v in entities.items()}\n",
        "\n",
        "    def summarize_text(self, text: str, entities: Dict[str, Any]) -> str:\n",
        "        # Dynamic length summarization\n",
        "        words = text.split()\n",
        "        max_length = min(100, max(30, int(len(words) * 0.3)))\n",
        "        min_length = max(20, int(len(words) * 0.1))\n",
        "\n",
        "        # Simplified summary generation\n",
        "        summary = \" \".join(words[:max_length])\n",
        "\n",
        "        # Enrich with entities\n",
        "        entity_summary = \" | \".join([\n",
        "            f\"{k.capitalize()}: {', '.join(v)}\"\n",
        "            for k, v in entities.items() if v\n",
        "        ])\n",
        "\n",
        "        return f\"{summary} | {entity_summary}\"\n",
        "\n",
        "    def process(self, text: str) -> Dict[str, Any]:\n",
        "        predicted_labels = self.predict_labels(text)\n",
        "        extracted_entities = self.extract_entities(text)\n",
        "        summary = self.summarize_text(text, extracted_entities)\n",
        "\n",
        "        return {\n",
        "            \"original_text\": text,\n",
        "            \"predicted_labels\": predicted_labels,\n",
        "            \"extracted_entities\": extracted_entities,\n",
        "            \"summary\": summary\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    pipeline = AdvancedInferencePipeline(\n",
        "        classifier_path=\"transformer_multi_label_classifier.pkl\",\n",
        "        domain_knowledge_path=\"domain_knowledge.json\"\n",
        "    )\n",
        "\n",
        "    text_snippet = (\n",
        "        \"CompetitorX offers advanced real-time analytics at a lower pricing model. \"\n",
        "        \"We need cost reduction to match their capabilities.\"\n",
        "    )\n",
        "\n",
        "    result = pipeline.process(text_snippet)\n",
        "    print(json.dumps(result, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQuqPCkjQS18",
        "outputId": "0b3828ad-a769-4698-b87f-e20448d17bb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       Competition       0.77      0.21      0.33        47\n",
            "          Negative       0.71      0.35      0.47        48\n",
            "         Objection       0.47      0.20      0.28        40\n",
            "          Positive       0.77      0.49      0.60        41\n",
            "Pricing Discussion       0.78      0.54      0.64        59\n",
            "          Security       0.53      0.21      0.30        39\n",
            "\n",
            "         micro avg       0.70      0.35      0.46       274\n",
            "         macro avg       0.67      0.33      0.44       274\n",
            "      weighted avg       0.68      0.35      0.45       274\n",
            "       samples avg       0.45      0.36      0.37       274\n",
            "\n",
            "Model, vectorizer, and label binarizer saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "from typing import Dict, Any\n",
        "import logging\n",
        "from functools import partial\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Advanced Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler('nlp_pipeline.log')\n",
        "    ]\n",
        ")\n",
        "\n",
        "class AdvancedCLI:\n",
        "    def __init__(self, inference_pipeline):\n",
        "        self.pipeline = inference_pipeline\n",
        "\n",
        "    def execute_task(self, task: str, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute specific NLP tasks with error handling and logging.\"\"\"\n",
        "        try:\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "                if task == \"classify\":\n",
        "                    future = executor.submit(self.pipeline.predict_labels, text)\n",
        "                    result = {\"predicted_labels\": future.result()}\n",
        "                elif task == \"extract\":\n",
        "                    future = executor.submit(self.pipeline.extract_entities, text)\n",
        "                    result = {\"extracted_entities\": future.result()}\n",
        "                elif task == \"summarize\":\n",
        "                    entities = self.pipeline.extract_entities(text)\n",
        "                    future = executor.submit(self.pipeline.summarize_text, text, entities)\n",
        "                    result = {\"summary\": future.result()}\n",
        "                elif task == \"all\":\n",
        "                    future = executor.submit(self.pipeline.process, text)\n",
        "                    result = future.result()\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid task: {task}\")\n",
        "\n",
        "            logging.info(f\"Task '{task}' completed successfully\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in task '{task}': {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @classmethod\n",
        "    def create_parser(cls) -> argparse.ArgumentParser:\n",
        "        \"\"\"Create advanced argument parser with rich configuration.\"\"\"\n",
        "        parser = argparse.ArgumentParser(\n",
        "            description=\"Advanced NLP Pipeline CLI\",\n",
        "            epilog=\"Process text with multi-modal NLP techniques\"\n",
        "        )\n",
        "\n",
        "        parser.add_argument(\n",
        "            \"--task\",\n",
        "            choices=[\"classify\", \"extract\", \"summarize\", \"all\"],\n",
        "            required=True,\n",
        "            help=\"Select NLP processing task\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--text\",\n",
        "            type=str,\n",
        "            required=True,\n",
        "            help=\"Text snippet for processing\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--input_file\",\n",
        "            type=str,\n",
        "            help=\"Process texts from input file (JSON/CSV)\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--output_file\",\n",
        "            type=str,\n",
        "            help=\"Save results to output file\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--verbose\",\n",
        "            action=\"store_true\",\n",
        "            help=\"Enable detailed logging\"\n",
        "        )\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def process_batch(self, input_file: str, task: str) -> list:\n",
        "        \"\"\"Process multiple texts from input file.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r') as f:\n",
        "                texts = json.load(f)\n",
        "\n",
        "            results = []\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "                task_func = partial(self.execute_task, task)\n",
        "                results = list(executor.map(task_func, texts))\n",
        "\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Batch processing error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_results(self, results, output_file: str):\n",
        "        \"\"\"Save processing results to file.\"\"\"\n",
        "        try:\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "            logging.info(f\"Results saved to {output_file}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving results: {e}\")\n",
        "\n",
        "def main():\n",
        "    from inference_pipeline import AdvancedInferencePipeline\n",
        "\n",
        "    pipeline = AdvancedInferencePipeline(\n",
        "        classifier_path=\"transformer_multi_label_classifier.pkl\",\n",
        "        domain_knowledge_path=\"domain_knowledge.json\"\n",
        "    )\n",
        "\n",
        "    cli = AdvancedCLI(pipeline)\n",
        "    parser = cli.create_parser()\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "\n",
        "    if args.input_file:\n",
        "        results = cli.process_batch(args.input_file, args.task)\n",
        "        if args.output_file:\n",
        "            cli.save_results(results, args.output_file)\n",
        "        else:\n",
        "            print(json.dumps(results, indent=2))\n",
        "    else:\n",
        "        result = cli.execute_task(args.task, args.text)\n",
        "        print(json.dumps(result, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZYFYD9GTzwh",
        "outputId": "9b23dfe8-4f83-45c7-f120-16b598e58e83"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated entity extraction completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/username/NLP-Project.git\n",
        "%cd NLP-Project\n"
      ],
      "metadata": {
        "id": "Ye4ewgDvdL6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}